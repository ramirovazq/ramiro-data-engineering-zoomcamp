{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb0e2763-9a08-4179-a2d3-fc0b87dfc7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60ff9e89-3041-435e-9ab1-323b0448c704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/06 23:30:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master('local[*]') \\\n",
    "        .appName('test')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffbc94a8-79fe-4c71-a6f6-ed6173486be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading csv file in spark\n",
    "df_green = spark.read \\\n",
    "     .option(\"header\",\"true\")\\\n",
    "     .csv(\"code/data/raw/green/2021/01/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3d27050-5842-4422-8d09-6b9db92c47cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2021-01-01 00:15:56|  2021-01-01 00:19:52|                 N|         1|          43|         151|              1|         1.01|        5.5|  0.5|    0.5|         0|           0|     null|                  0.3|         6.8|           2|        1|                   0|\n",
      "|       2| 2021-01-01 00:25:59|  2021-01-01 00:34:44|                 N|         1|         166|         239|              1|         2.53|         10|  0.5|    0.5|      2.81|           0|     null|                  0.3|       16.86|           1|        1|                2.75|\n",
      "|       2| 2021-01-01 00:45:57|  2021-01-01 00:51:55|                 N|         1|          41|          42|              1|         1.12|          6|  0.5|    0.5|         1|           0|     null|                  0.3|         8.3|           1|        1|                   0|\n",
      "|       2| 2020-12-31 23:57:51|  2021-01-01 00:04:56|                 N|         1|         168|          75|              1|         1.99|          8|  0.5|    0.5|         0|           0|     null|                  0.3|         9.3|           2|        1|                   0|\n",
      "|       2| 2021-01-01 00:16:36|  2021-01-01 00:16:40|                 N|         2|         265|         265|              3|          .00|        -52|    0|   -0.5|         0|           0|     null|                 -0.3|       -52.8|           3|        1|                   0|\n",
      "|       2| 2021-01-01 00:16:36|  2021-01-01 00:16:40|                 N|         2|         265|         265|              3|          .00|         52|    0|    0.5|         0|           0|     null|                  0.3|        52.8|           2|        1|                   0|\n",
      "|       2| 2021-01-01 00:19:14|  2021-01-01 00:19:21|                 N|         5|         265|         265|              1|          .00|        180|    0|      0|     36.06|           0|     null|                  0.3|      216.36|           1|        2|                   0|\n",
      "|       2| 2021-01-01 00:26:31|  2021-01-01 00:28:50|                 N|         1|          75|          75|              6|          .45|        3.5|  0.5|    0.5|      0.96|           0|     null|                  0.3|        5.76|           1|        1|                   0|\n",
      "|       2| 2021-01-01 00:57:46|  2021-01-01 00:57:57|                 N|         1|         225|         225|              1|          .00|        2.5|  0.5|    0.5|         0|           0|     null|                  0.3|         3.8|           2|        1|                   0|\n",
      "|       2| 2021-01-01 00:58:32|  2021-01-01 01:32:34|                 N|         1|         225|         265|              1|        12.19|         38|  0.5|    0.5|      2.75|           0|     null|                  0.3|       42.05|           1|        1|                   0|\n",
      "|       2| 2021-01-01 00:31:14|  2021-01-01 00:55:07|                 N|         1|         244|         244|              2|         3.39|         18|  0.5|    0.5|         0|           0|     null|                  0.3|        19.3|           2|        1|                   0|\n",
      "|       2| 2021-01-01 00:08:50|  2021-01-01 00:21:56|                 N|         1|          75|         213|              1|         6.69|       19.5|  0.5|    0.5|         0|           0|     null|                  0.3|        20.8|           2|        1|                   0|\n",
      "|       2| 2021-01-01 00:35:13|  2021-01-01 00:44:44|                 N|         1|          74|         238|              1|         2.34|         10|  0.5|    0.5|         0|           0|     null|                  0.3|       14.05|           1|        1|                2.75|\n",
      "|       2| 2021-01-01 00:39:57|  2021-01-01 00:55:25|                 N|         1|          74|          60|              1|         5.48|         18|  0.5|    0.5|         0|           0|     null|                  0.3|        19.3|           2|        1|                   0|\n",
      "|       1| 2021-01-01 00:51:27|  2021-01-01 00:57:20|                 N|         1|          42|          41|              2|          .90|          6|  0.5|    0.5|         0|           0|     null|                  0.3|         7.3|           1|        1|                   0|\n",
      "|       2| 2021-01-01 00:29:05|  2021-01-01 00:29:07|                 N|         5|          42|         264|              1|          .00|         10|    0|      0|      2.06|           0|     null|                  0.3|       12.36|           1|        2|                   0|\n",
      "|       2| 2021-01-01 00:32:07|  2021-01-01 00:42:54|                 N|         1|          74|         116|              1|         2.08|        9.5|  0.5|    0.5|      2.16|           0|     null|                  0.3|       12.96|           1|        1|                   0|\n",
      "|       2| 2021-01-01 00:49:59|  2021-01-01 01:05:01|                 N|         1|         116|         143|              1|         4.64|       16.5|  0.5|    0.5|      5.14|           0|     null|                  0.3|       25.69|           1|        1|                2.75|\n",
      "|       2| 2021-01-01 00:07:20|  2021-01-01 00:12:01|                 N|         1|          75|          42|              1|         1.68|        6.5|  0.5|    0.5|         0|           0|     null|                  0.3|         7.8|           2|        1|                   0|\n",
      "|       2| 2021-01-01 00:25:54|  2021-01-01 00:28:20|                 N|         1|          74|          75|              1|          .68|          4|  0.5|    0.5|         0|           0|     null|                  0.3|         5.3|           2|        1|                   0|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7173eca-80a6-4676-9318-032884a6f889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- ehail_fee: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- trip_type: string (nullable = true)\n",
      " |-- congestion_surcharge: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green.printSchema() # we can see not has an schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8208706e-ca93-48f3-8b44-5aeb3121fc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VendorID                   int64\n",
       "lpep_pickup_datetime      object\n",
       "lpep_dropoff_datetime     object\n",
       "store_and_fwd_flag        object\n",
       "RatecodeID                 int64\n",
       "PULocationID               int64\n",
       "DOLocationID               int64\n",
       "passenger_count            int64\n",
       "trip_distance            float64\n",
       "fare_amount              float64\n",
       "extra                    float64\n",
       "mta_tax                  float64\n",
       "tip_amount               float64\n",
       "tolls_amount             float64\n",
       "ehail_fee                float64\n",
       "improvement_surcharge    float64\n",
       "total_amount             float64\n",
       "payment_type               int64\n",
       "trip_type                  int64\n",
       "congestion_surcharge     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_green_pandas = pd.read_csv(\"code/data/raw/green/2021/01/green_tripdata_2021_01.csv.gz\", nrows=1000)\n",
    "df_green_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c5bc2f7-f0c5-4e96-89fc-ab7639707faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_pandas = df_green_pandas.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dbff9dd-60cf-48e4-874d-28dc0fffe144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating error right now so doing explicitly\n",
    "#spark.createDataFrame(df_green_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69ced145-9ef1-41bd-bc31-516b00f08178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', StringType(), True), StructField('lpep_pickup_datetime', StringType(), True), StructField('lpep_dropoff_datetime', StringType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('RatecodeID', StringType(), True), StructField('PULocationID', StringType(), True), StructField('DOLocationID', StringType(), True), StructField('passenger_count', StringType(), True), StructField('trip_distance', StringType(), True), StructField('fare_amount', StringType(), True), StructField('extra', StringType(), True), StructField('mta_tax', StringType(), True), StructField('tip_amount', StringType(), True), StructField('tolls_amount', StringType(), True), StructField('ehail_fee', StringType(), True), StructField('improvement_surcharge', StringType(), True), StructField('total_amount', StringType(), True), StructField('payment_type', StringType(), True), StructField('trip_type', StringType(), True), StructField('congestion_surcharge', StringType(), True)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffe2e549-c8f7-4ba6-89a9-a25c621da037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99614024-e8b3-4194-aada-0720622c3af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_schema = types.StructType([\n",
    "        types.StructField('VendorID', types.IntegerType(), True), \n",
    "        types.StructField('lpep_pickup_datetime', types.TimestampType(), True), \n",
    "        types.StructField('lpep_dropoff_datetime', types.TimestampType(), True), \n",
    "        types.StructField('store_and_fwd_flag', types.StringType(), True), \n",
    "        types.StructField('RatecodeID', types.IntegerType(), True), \n",
    "        types.StructField('PULocationID', types.IntegerType(), True), \n",
    "        types.StructField('DOLocationID', types.IntegerType(), True), \n",
    "        types.StructField('passenger_count', types.IntegerType(), True), \n",
    "        types.StructField('trip_distance', types.DoubleType(), True), \n",
    "        types.StructField('fare_amount', types.DoubleType(), True), \n",
    "        types.StructField('extra', types.DoubleType(), True), \n",
    "        types.StructField('mta_tax', types.DoubleType(), True),\n",
    "        types.StructField('tip_amount', types.DoubleType(), True), \n",
    "        types.StructField('tolls_amount', types.DoubleType(), True), \n",
    "        types.StructField('ehail_fee', types.DoubleType(), True), \n",
    "        types.StructField('improvement_surcharge', types.DoubleType(), True), \n",
    "        types.StructField('total_amount', types.DoubleType(), True), \n",
    "        types.StructField('payment_type', types.IntegerType(), True), \n",
    "        types.StructField('trip_type', types.IntegerType(), True), \n",
    "        types.StructField('congestion_surcharge', types.DoubleType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c9828fa-fa30-4fa4-a2d5-a3f7708f1880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading csv file in spark\n",
    "df_green = spark.read \\\n",
    "     .option(\"header\",\"true\")\\\n",
    "     .schema(green_schema)\\\n",
    "     .csv(\"code/data/raw/green/2021/01/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1144e96-027f-4452-b59f-921c90864579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- trip_type: integer (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ced23b76-3556-4967-9da6-57d0e10ee70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_schema = types.StructType([\n",
    "        types.StructField('VendorID', types.IntegerType(), True), \n",
    "        types.StructField('tpep_pickup_datetime', types.TimestampType(), True), \n",
    "        types.StructField('tpep_dropoff_datetime', types.TimestampType(), True), \n",
    "        types.StructField('passenger_count', types.IntegerType(), True), \n",
    "        types.StructField('trip_distance', types.DoubleType(), True), \n",
    "        types.StructField('RatecodeID', types.IntegerType(), True), \n",
    "        types.StructField('store_and_fwd_flag', types.StringType(), True), \n",
    "        types.StructField('PULocationID', types.IntegerType(), True), \n",
    "        types.StructField('DOLocationID', types.IntegerType(), True), \n",
    "        types.StructField('payment_type', types.IntegerType(), True),\n",
    "        types.StructField('fare_amount', types.DoubleType(), True), \n",
    "        types.StructField('extra', types.DoubleType(), True), \n",
    "        types.StructField('mta_tax', types.DoubleType(), True),\n",
    "        types.StructField('tip_amount', types.DoubleType(), True), \n",
    "        types.StructField('tolls_amount', types.DoubleType(), True), \n",
    "        types.StructField('improvement_surcharge', types.DoubleType(), True), \n",
    "        types.StructField('total_amount', types.DoubleType(), True),\n",
    "        types.StructField('congestion_surcharge', types.DoubleType(), True),       \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33d72828-25b9-445e-835d-003d7de2515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark.read \\\n",
    "     .option(\"header\",\"true\")\\\n",
    "     .schema(yellow_schema)\\\n",
    "     .csv(\"code/data/raw/yellow/2021/01/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff4c4730-53fd-4a09-a6e5-4be27f39b14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_yellow.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec889fd3-54d2-40b0-877d-b8a3e42a4941",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2020\n",
    "\n",
    "for month in range(1, 13):\n",
    "    print(f'processing data for {year}/{month}')\n",
    "\n",
    "    input_path = f'code/data/raw/green/{year}/{month:02d}/'\n",
    "    output_path = f'code/data/pq/green/{year}/{month:02d}/'\n",
    "\n",
    "    df_green = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .schema(green_schema) \\\n",
    "        .csv(input_path)\n",
    "\n",
    "    df_green \\\n",
    "        .repartition(4) \\\n",
    "        .write.parquet(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cee739a2-092a-47d2-8ffb-c6c437ce7c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2021/1\n",
      "processing data for 2021/2\n",
      "processing data for 2021/3\n",
      "processing data for 2021/4\n",
      "processing data for 2021/5\n",
      "processing data for 2021/6\n",
      "processing data for 2021/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2021/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/home/ramisu/ramiro-data-engineering-zoomcamp/5_batch/code/data/raw/green/2021/08",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 12\u001b[0m\n\u001b[1;32m      6\u001b[0m input_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode/data/raw/green/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode/data/pq/green/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m df_green \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread \\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mschema(green_schema) \\\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;241m.\u001b[39mcsv(input_path)\n\u001b[1;32m     14\u001b[0m df_green \\\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m4\u001b[39m) \\\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mparquet(output_path)\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/readwriter.py:535\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/home/ramisu/ramiro-data-engineering-zoomcamp/5_batch/code/data/raw/green/2021/08"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "year = 2021\n",
    "\n",
    "for month in range(1, 13):\n",
    "    print(f'processing data for {year}/{month}')\n",
    "\n",
    "    input_path = f'code/data/raw/green/{year}/{month:02d}/'\n",
    "    output_path = f'code/data/pq/green/{year}/{month:02d}/'\n",
    "\n",
    "    df_green = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .schema(green_schema) \\\n",
    "        .csv(input_path)\n",
    "\n",
    "    df_green \\\n",
    "        .repartition(4) \\\n",
    "        .write.parquet(output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51ca31ff-365d-47cc-bb6a-5b4c3f2fceeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2020/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "year = 2020\n",
    "\n",
    "for month in range(1, 13):\n",
    "    print(f'processing data for {year}/{month}')\n",
    "\n",
    "    input_path = f'code/data/raw/yellow/{year}/{month:02d}/'\n",
    "    output_path = f'code/data/pq/yellow/{year}/{month:02d}/'\n",
    "\n",
    "    df_yellow = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .schema(yellow_schema) \\\n",
    "        .csv(input_path)\n",
    "\n",
    "    df_yellow \\\n",
    "        .repartition(4) \\\n",
    "        .write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e86d9-fea2-4767-98d0-71d9471e901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2021\n",
    "\n",
    "for month in range(1, 13):\n",
    "    print(f'processing data for {year}/{month}')\n",
    "\n",
    "    input_path = f'code/data/raw/yellow/{year}/{month:02d}/'\n",
    "    output_path = f'code/data/pq/yellow/{year}/{month:02d}/'\n",
    "\n",
    "    df_yellow = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .schema(yellow_schema) \\\n",
    "        .csv(input_path)\n",
    "\n",
    "    df_yellow \\\n",
    "        .repartition(4) \\\n",
    "        .write.parquet(output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
